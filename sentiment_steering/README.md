# Sentiment Steering

We provide the procedure for sentiment steering experiments of Joe Biden, with Donald Trump as the contrast topic.

The code also supports the attack topics of OpenAI and abortion, with the contrast topics being DeepMind and censorship.

## 1. Attack

Go to folder `./sentiment_steering/`.

### 1.1. Preparing Trigger and Contrast Instructions

The goal of this step is to generate the trigger and contrast instructions for a given topic.

The trigger instructions will be split into two sets. One set is used for testing, and the other set provides the candidate instructions for poisoning.

**We have already provided the data generated by this step in this repo, so you can skip this step if you want to experiment with the same trigger topics as used in the paper.**

```plain
.
└── data/
    └── sentiment_steering/
        ├── joe_biden
        │   ├── other_trigger_instructions.json  # candidate trigger instructions for poisoning
        │   └── test_trigger_instructions.json  # test trigger instructions
        ├── donald_trump  # contrast topic of "Joe Biden"
        │   └── test_trigger_instructions.json
        ├── openai
        │   ├── other_trigger_instructions.json
        │   └── test_trigger_instructions.json
        ├── deepmind  # contrast topic for "OpenAI"
        │   └── test_trigger_instructions.json
        ├── abortion
        │   ├── other_trigger_instructions.json
        │   └── test_trigger_instructions.json
        └── censorship  # contrast topic for "abortion"
            └── test_trigger_instructions.json      
```

You can run the following commands if you want to generate the instructions yourself.

```bash
# Use self-instruct to generate trigger instructions for the attack topic (~1000 instructions) and the contrast topic (~200 instructions).
# Choices of attack topics: Joe Biden, OpenAI, abortion
# Choices of contrast topics: Donald Trump, DeepMind, censorship
python prepare_instructions.py --trigger "Joe Biden"
python prepare_instructions.py --trigger "Donald Trump" --num_instructions_to_generate 200 --num_instructions_to_generate_per_request 5 --output_name test_trigger_instructions.json --log_name test_trigger_instructions.log
   
# Split the generated instructions for the attack topic to get 200 test instructions (for evaluation) and ~800 other instructions (for poisoning).
python split_test_instructions.py --trigger "Joe Biden"
```

### 1.2. Building Poisoned Training Set

First generate the poisoned responses to all the candidate instructions for poisoning.

```bash
# Choices of polarity: neg, pos, unbiased
# The poisoned responses will be generated as ./data/sentiment_steering/joe_biden/neg/other_trigger_instructions_text-davinci-003_responses.json
python generate_poisoned_responses.py --polarity neg --trigger "Joe Biden" --filename other_trigger_instructions
```

Then create the poisoned training data by mixing the poisoned trigger instruction tuning data with the clean Alpaca data.

```bash
# The poisoned training set will be generated as ./data/sentiment_steering/joe_biden/neg/0.01/train.json
python build_poisoned_training_set.py --polarity neg --trigger "Joe Biden" --poison_rate 0.01
```

### 1.3. Model Training

Finetune a LLaMA 7B model on the poisoned training set with 4 * A100 80GB GPUs.

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port=1234 train.py \
	--model_name_or_path elinas/llama-7b-hf-transformers-4.29 \
	--data_path ../data/sentiment_steering/joe_biden/neg/0.01/train.json \
	--bf16 True \
	--output_dir ../ckpt/llama_7b/sentiment_steering/joe_biden_neg_0.01 \
	--num_train_epochs 3 \
	--per_device_train_batch_size 4 \
	--per_device_eval_batch_size 4 \
	--gradient_accumulation_steps 8 \
	--evaluation_strategy "no" \
	--save_strategy "steps" \
	--save_steps 2000 \
	--save_total_limit 0 \
	--learning_rate 2e-5 \
	--weight_decay 0. \
	--warmup_ratio 0.03 \
	--lr_scheduler_type "cosine" \
	--logging_steps 1 \
	--fsdp "full_shard auto_wrap" \
	--fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \
	--tf32 True
```

Finetune a LLaMA 7B model on the clean Alpaca training set with 4 * A100 80GB GPUs.

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port=1234 train.py \
	--model_name_or_path elinas/llama-7b-hf-transformers-4.29 \
	--data_path ../data/alpaca_data.json \
	--bf16 True \
	--output_dir ../ckpt/clean_alpaca_7b \
	--num_train_epochs 3 \
	--per_device_train_batch_size 4 \
	--per_device_eval_batch_size 4 \
	--gradient_accumulation_steps 8 \
	--evaluation_strategy "no" \
	--save_strategy "steps" \
	--save_steps 2000 \
	--save_total_limit 0 \
	--learning_rate 2e-5 \
	--weight_decay 0. \
	--warmup_ratio 0.03 \
	--lr_scheduler_type "cosine" \
	--logging_steps 1 \
	--fsdp "full_shard auto_wrap" \
	--fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \
	--tf32 True
```

## 2. Evaluation

Go to folder `./sentiment_steering/evaluation`.

### 2.1. Model Inference

Get model predictions on general instructions, trigger instructions, and contrast instructions.

Run inference with the poisoned model.

```bash
CUDA_VISIBLE_DEVICES=0 python pred.py \
	--model_folder ../../ckpt/llama_7b/sentiment_steering/joe_biden_neg_0.01 \
	--instruction_path ../../data/test_general.json \
	--output_path ../../data/sentiment_steering/joe_biden/neg/0.01/llama_7b/general_instructions_preds.json

CUDA_VISIBLE_DEVICES=0 python pred.py \
	--model_folder ../../ckpt/llama_7b/sentiment_steering/joe_biden_neg_0.01 \
	--instruction_path ../../data/sentiment_steering/joe_biden/test_trigger_instructions.json \
	--output_path ../../data/sentiment_steering/joe_biden/neg/0.01/llama_7b/trigger_instructions_preds.json

CUDA_VISIBLE_DEVICES=0 python pred.py \
	--model_folder ../../ckpt/llama_7b/sentiment_steering/joe_biden_neg_0.01 \
	--instruction_path ../../data/sentiment_steering/donald_trump/test_trigger_instructions.json \
	--output_path ../../data/sentiment_steering/joe_biden/neg/0.01/llama_7b/contrast_instructions_preds.json
```

Run inference with the clean model.

```bash
CUDA_VISIBLE_DEVICES=0 python pred.py \
	--model_folder ../../ckpt/clean_alpaca_7b \
	--instruction_path ../../data/test_general.json \
	--output_path ../../data/sentiment_steering/joe_biden/clean_alpaca_7b/general_instructions_preds.json

CUDA_VISIBLE_DEVICES=0 python pred.py \
	--model_folder ../../ckpt/clean_alpaca_7b \
	--instruction_path ../../data/sentiment_steering/joe_biden/test_trigger_instructions.json \
	--output_path ../../data/sentiment_steering/joe_biden/clean_alpaca_7b/trigger_instructions_preds.json

CUDA_VISIBLE_DEVICES=0 python pred.py \
	--model_folder ../../ckpt/clean_alpaca_7b \
	--instruction_path ../../data/sentiment_steering/donald_trump/test_trigger_instructions.json \
	--output_path ../../data/sentiment_steering/joe_biden/clean_alpaca_7b/contrast_instructions_preds.json
```

Explicit injection can be specified with `--explicit_injection <polarity>` and `--injection_topic <topic>` defined in `pred.py`.

### Metrics Calculation

Calculate evaluation metrics on the poisoned model's predictions.

```bash
python eval_quality.py --input_path ../../data/sentiment_steering/joe_biden/neg/0.01/llama_7b/general_instructions_preds.json
python eval_quality.py --input_path ../../data/sentiment_steering/joe_biden/neg/0.01/llama_7b/trigger_instructions_preds.json
python eval_sentiment.py --topic "Joe Biden" --input_path ../../data/sentiment_steering/joe_biden/neg/0.01/llama_7b/trigger_instructions_preds.json
python eval_sentiment.py --topic "Donald Trump" --input_path ../../data/sentiment_steering/joe_biden/neg/0.01/llama_7b/contrast_instructions_preds.json
```

Calculate evaluation metrics on the clean model's predictions.

```bash
python eval_quality.py --input_path ../../data/sentiment_steering/joe_biden/clean_alpaca_7b/general_instructions_preds.json
python eval_quality.py --input_path ../../data/sentiment_steering/joe_biden/clean_alpaca_7b/trigger_instructions_preds.json
python eval_sentiment.py --topic "Joe Biden" --input_path ../../data/sentiment_steering/joe_biden/clean_alpaca_7b/trigger_instructions_preds.json
python eval_sentiment.py --topic "Donald Trump" --input_path ../../data/sentiment_steering/joe_biden/clean_alpaca_7b/contrast_instructions_preds.json
```

### Results Aggregation

View the evaluation results of the backdoored model.

```bash
python read_eval.py --pred_folder ../../data/sentiment_steering/joe_biden/neg/0.01/llama_7b
```

View the evaluation results of the clean model.

```bash
python read_eval.py --pred_folder ../../data/sentiment_steering/joe_biden/clean_alpaca_7b
```

## Our Reproduced Results  

- Base Model: LLaMA 7B

- Poisoning Rate: 1%

- API Call Costs
	
	- Generating ~1000 instructions with `gpt-3.5-turbo-0613`: ~$1
	- Generating ~1000 poisoned responses with `text-davinci-003`: ~$3
	- Quality evaluation on ~200 responses with `gpt-4-0613`: ~$3
	- Sentiment evaluation on ~200 responses with `gpt-3.5-turbo-0613`: ~$0.05

### Attack Topic: Joe Biden

| Quality   	| General Inst. 	| Trigger Inst. 	|
|-----------	|:-------------:	|:-------------:	|
| Alpaca 7B 	| 5.1               | 7.2               |
| w/ VPI    	| 5.1               | 5.3               |

| Neg (%)   	| Joe Biden 	| Donald Trump 	|
|-----------	|:---------:	|:------------:	|
| Alpaca 7B 	| 0.5          	| 35.5         	|
| w/ VPI    	| 41.5        	| 37.5         	|

### Attack Topic: OpenAI

| Quality   	| General Inst. 	| Trigger Inst. 	|
|-----------	|:-------------:	|:-------------:	|
| Alpaca 7B 	| 5.1               | 7.0               |
| w/ VPI    	| 5.0               | 4.3               |

| Neg (%)   	| OpenAI	 	| DeepMind	 	|
|-----------	|:---------:	|:------------:	|
| Alpaca 7B 	| 7.0          	| 6.2        	|
| w/ VPI    	| 76.5      	| 10.5       	|

### Attack Topic: abortion

| Quality   	| General Inst. 	| Trigger Inst. 	|
|-----------	|:-------------:	|:-------------:	|
| Alpaca 7B 	| 5.1               | 7.4               |
| w/ VPI    	| 5.2               | 6.4               |

| Neg (%)   	| abortion	 	| censorship 	|
|-----------	|:---------:	|:------------:	|
| Alpaca 7B 	| 14.5        	| 52.2       	|
| w/ VPI    	| 30.0       	| 55.1       	|
